# ğŸ›£ï¸ Lane Detection with U-Net

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![TuSimple](https://img.shields.io/badge/Dataset-TuSimple-orange.svg)](https://github.com/TuSimple/tusimple-benchmark)

A deep learning implementation for autonomous vehicle lane detection using U-Net architecture and the TuSimple benchmark dataset. This project demonstrates semantic segmentation for real-time lane marking detection in highway driving scenarios.

## ğŸ“‹ Table of Contents

- [ğŸ” Overview](#-overview)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸ“Š Dataset](#-dataset)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ§  Model Details](#-model-details)
- [ğŸ“š Code Walkthrough](#-code-walkthrough)
- [ğŸ”§ Training Process](#-training-process)
- [ğŸ“ˆ Performance](#-performance)
- [ğŸ› ï¸ Customization](#ï¸-customization)
- [ğŸ“– References](#-references)

## ğŸ” Overview

Lane detection is a critical component of Advanced Driver Assistance Systems (ADAS) and autonomous vehicles. This project implements a simplified U-Net architecture to perform pixel-level binary segmentation, identifying lane markings in road images.

### Key Features

- **ğŸ¯ Semantic Segmentation**: Pixel-level lane classification using U-Net
- **ğŸƒ Real-time Ready**: Lightweight architecture optimized for inference speed
- **ğŸ“Š Benchmark Dataset**: Trained and tested on TuSimple dataset
- **ğŸ”„ End-to-End Pipeline**: Complete data loading, training, and visualization
- **ğŸ“± Modular Design**: Clean, extensible codebase with comprehensive documentation

### Technical Highlights

- **Architecture**: Simplified U-Net with skip connections
- **Loss Function**: Binary Cross-Entropy with Logits Loss
- **Optimizer**: Adam with learning rate 1e-4
- **Input Resolution**: 256Ã—512 pixels (optimized for speed/accuracy balance)
- **Output**: Binary segmentation mask (lane vs. background)

## ğŸ—ï¸ Architecture

### U-Net Overview

Our simplified U-Net consists of two main components:

#### ğŸ”½ Encoder (Contracting Path)
- **Purpose**: Capture context and extract hierarchical features
- **Operations**: Convolution + pooling for downsampling
- **Feature Evolution**: 3 â†’ 32 â†’ 64 channels

#### ğŸ”¼ Decoder (Expanding Path)  
- **Purpose**: Precise localization and upsampling
- **Operations**: Transposed convolution + concatenation
- **Feature Evolution**: 64 â†’ 32 â†’ 1 channel

#### ğŸ”— Skip Connections
- **Purpose**: Preserve fine-grained spatial information
- **Implementation**: Concatenate encoder features with decoder features
- **Benefit**: Combines global context with local details

```
Input (3, 256, 512)
        â†“
    DoubleConv (32)
        â†“ â†â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â”
    MaxPool2d          â”‚ Skip Connection
        â†“              â”‚
    DoubleConv (64)    â”‚
        â†“              â”‚
    ConvTranspose2d    â”‚
        â†“              â”‚
    Concatenate â†â€•â€•â€•â€•â€•â€•â”˜
        â†“
    DoubleConv (32)
        â†“
    Conv2d (1)
        â†“
Output (1, 256, 512)
```

## ğŸ“Š Dataset

### TuSimple Lane Detection Dataset

The TuSimple dataset is a benchmark for lane detection research, featuring:

- **ğŸš— Real Highway Data**: Collected from actual highway driving
- **ğŸ“ Resolution**: Original 720Ã—1280 pixels (resized to 256Ã—512 for training)
- **ğŸ·ï¸ Annotation Format**: JSON files with polynomial lane representations
- **ğŸ“‚ Structure**: Separate training and testing splits

#### Dataset Statistics
- **Training Set**: ~3,600 images
- **Test Set**: ~2,800 images  
- **Lane Types**: 2-4 lanes per image
- **Conditions**: Various lighting and weather conditions

#### Annotation Format
```json
{
  "lanes": [
    [1080, 1040, 1000, ...],  // Lane 1 x-coordinates
    [1500, 1460, 1420, ...],  // Lane 2 x-coordinates
    [-2, -2, -2, ...]         // Lane 3 (invisible)
  ],
  "h_samples": [160, 170, 180, ..., 710],  // y-coordinates
  "raw_file": "clips/0530/1492626047222176976_0/20.jpg"
}
```

## ğŸš€ Quick Start

### Prerequisites

```bash
# Create virtual environment
python -m venv lane_detection_env
source lane_detection_env/bin/activate  # On Windows: lane_detection_env\Scripts\activate

# Install dependencies
pip install torch torchvision opencv-python matplotlib numpy
```

### Dataset Setup

1. **Download TuSimple Dataset**:
   ```bash
   # Download from: https://github.com/TuSimple/tusimple-benchmark
   # Extract to: tusimple/TUSimple/
   ```

2. **Verify Structure**:
   ```
   tusimple/TUSimple/
   â”œâ”€â”€ train_set/
   â”‚   â”œâ”€â”€ label_data_0313.json
   â”‚   â”œâ”€â”€ label_data_0531.json
   â”‚   â”œâ”€â”€ label_data_0601.json
   â”‚   â””â”€â”€ clips/
   â””â”€â”€ test_set/
       â”œâ”€â”€ test_label.json
       â””â”€â”€ clips/
   ```

### Training

```bash
# Start training
python train.py

# Monitor progress
# Training progress will be displayed in terminal
# Model checkpoints saved as model_epoch_*.pth
```

### Testing/Visualization

```bash
# Visualize dataset samples
python test_loader.py

# This will display:
# - Original road image
# - Generated binary lane mask
```

## ğŸ“ Project Structure

```
lane_detection/
â”œâ”€â”€ ğŸ“„ README.md              # This comprehensive guide
â”œâ”€â”€ ğŸ§  model.py               # U-Net architecture implementation
â”œâ”€â”€ ğŸ“Š dataset_loader.py      # TuSimple dataset handling
â”œâ”€â”€ ğŸ¯ train.py               # Training script with full pipeline
â”œâ”€â”€ ğŸ‘ï¸ test_loader.py         # Visualization and testing utilities
â”œâ”€â”€ ğŸ“ tusimple/              # TuSimple dataset directory
â”‚   â””â”€â”€ TUSimple/
â”‚       â”œâ”€â”€ train_set/        # Training images and annotations
â”‚       â””â”€â”€ test_set/         # Testing images and annotations
â””â”€â”€ ğŸ’¾ model_epoch_*.pth      # Saved model checkpoints
```

## ğŸ§  Model Details

### DoubleConv Block

The fundamental building block of our U-Net:

```python
class DoubleConv(nn.Module):
    """
    Performs: Conv2d â†’ ReLU â†’ Conv2d â†’ ReLU
    
    Benefits:
    - Feature extraction at each resolution level
    - Maintains spatial dimensions (padding=1)
    - Non-linear activation for complex pattern learning
    """
```

**Key Characteristics**:
- **Kernel Size**: 3Ã—3 (optimal for local feature extraction)
- **Padding**: 1 (preserves spatial dimensions)
- **Activation**: ReLU (prevents vanishing gradients)
- **Memory Optimization**: inplace=True for ReLU

### SimpleUNet Architecture

#### Layer-by-Layer Breakdown

| Layer | Input Shape | Output Shape | Purpose |
|-------|-------------|--------------|---------|
| `enc1` | (B, 3, 256, 512) | (B, 32, 256, 512) | Extract low-level features |
| `pool` | (B, 32, 256, 512) | (B, 32, 128, 256) | Reduce spatial dimensions |
| `enc2` | (B, 32, 128, 256) | (B, 64, 128, 256) | Extract high-level features |
| `up` | (B, 64, 128, 256) | (B, 32, 256, 512) | Restore spatial dimensions |
| `concat` | [(B, 32, 256, 512), (B, 32, 256, 512)] | (B, 64, 256, 512) | Combine features |
| `dec1` | (B, 64, 256, 512) | (B, 32, 256, 512) | Process combined features |
| `outc` | (B, 32, 256, 512) | (B, 1, 256, 512) | Generate final prediction |

*B = Batch Size*

#### Skip Connection Benefits

1. **ğŸ” Gradient Flow**: Enables training of deeper networks
2. **ğŸ“ Spatial Preservation**: Maintains fine-grained location information
3. **ğŸ¯ Multi-Scale Features**: Combines low-level and high-level representations
4. **âš¡ Faster Convergence**: Accelerates training process

## ğŸ“š Code Walkthrough

### 1. Dataset Loading (`dataset_loader.py`)

#### Key Components:

**TuSimpleDataset Class**:
```python
class TuSimpleDataset(Dataset):
    def __init__(self, root_dir, split='train', transform=None):
        # Loads JSON annotations and discovers image files
        
    def __getitem__(self, idx):
        # 1. Load image from file path
        # 2. Extract lane coordinates from JSON
        # 3. Generate binary mask from coordinates
        # 4. Apply preprocessing transforms
        # 5. Return (image, mask) pair
```

**Mask Generation Process**:
1. **Initialize**: Create zero-filled mask matching image dimensions
2. **Iterate**: Process each lane's coordinate points
3. **Draw**: Use `cv2.circle()` to create thick lane lines (radius=2)
4. **Convert**: Transform mask alongside image for consistency

**Transform Pipeline**:
- `ToPILImage()`: Convert numpy arrays to PIL format
- `Resize((256, 512))`: Standardize input dimensions
- `ToTensor()`: Convert to PyTorch tensors and normalize [0,1]

### 2. Model Architecture (`model.py`)

#### DoubleConv Implementation:
```python
def forward(self, x):
    # Sequential execution:
    # x â†’ Conv2d â†’ ReLU â†’ Conv2d â†’ ReLU â†’ output
    return self.net(x)
```

#### U-Net Forward Pass:
```python
def forward(self, x):
    # Encoder path
    e1 = self.enc1(x)           # Low-level features
    e2 = self.enc2(self.pool(e1))  # High-level features
    
    # Decoder path
    d1 = self.up(e2)            # Upsample
    d1 = torch.cat([d1, e1], dim=1)  # Skip connection
    out = self.outc(self.dec1(d1))   # Final prediction
    
    return out  # Raw logits (apply sigmoid for probabilities)
```

### 3. Training Pipeline (`train.py`)

#### Training Loop Components:

**Data Loading**:
```python
for imgs, masks in train_loader:
    imgs = imgs.to(torch.float32)   # Ensure correct dtype
    masks = masks.to(torch.float32) # Match network expectations
```

**Forward Pass**:
```python
preds = model(imgs).squeeze(1)  # Remove channel dim for BCE loss
loss = criterion(preds, masks)  # Compute binary cross-entropy
```

**Backpropagation**:
```python
optimizer.zero_grad()  # Clear previous gradients
loss.backward()        # Compute gradients
optimizer.step()       # Update parameters
```

**Checkpointing**:
```python
torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')
```

### 4. Visualization (`test_loader.py`)

#### Sample Display Process:
1. **Load**: Single batch from DataLoader
2. **Convert**: Tensor format to numpy for matplotlib
3. **Display**: Side-by-side image and mask visualization
4. **Format**: RGB image + grayscale binary mask

## ğŸ”§ Training Process

### Hyperparameter Selection

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| **Batch Size** | 4 | Balance between memory usage and gradient stability |
| **Learning Rate** | 1e-4 | Conservative rate for stable convergence |
| **Epochs** | 10 | Initial training; monitor for early stopping |
| **Optimizer** | Adam | Adaptive learning rates, good for computer vision |
| **Loss Function** | BCEWithLogitsLoss | Numerical stability + binary classification |

### Loss Function Deep Dive

**BCEWithLogitsLoss Benefits**:
- **ğŸ§® Numerical Stability**: Combines sigmoid + BCE in single operation
- **âš–ï¸ Class Balance**: Handles imbalanced lane/background pixels
- **ğŸ“Š Probabilistic Output**: Provides confidence scores
- **ğŸ¯ Gradient Quality**: Smooth gradients for optimization

Mathematical formulation:
```
Loss = -[y * log(Ïƒ(x)) + (1-y) * log(1-Ïƒ(x))]
where Ïƒ(x) = 1/(1 + e^(-x))
```

### Training Monitoring

**Key Metrics to Track**:
- **ğŸ“‰ Loss Curves**: Training loss per epoch
- **â±ï¸ Training Time**: Batch processing speed
- **ğŸ’¾ Memory Usage**: GPU/CPU utilization
- **ğŸ¯ Convergence**: Loss stabilization indicators

**Expected Training Progression**:
1. **Epochs 1-3**: Rapid loss decrease (learning basic features)
2. **Epochs 4-7**: Moderate improvement (refining boundaries)
3. **Epochs 8-10**: Fine-tuning (convergence phase)

## ğŸ“ˆ Performance

### Evaluation Metrics

For lane detection tasks, consider these metrics:

**Pixel-Level Metrics**:
- **ğŸ¯ IoU (Intersection over Union)**: Overlap between predicted and ground truth
- **ğŸ“Š F1-Score**: Harmonic mean of precision and recall
- **âœ… Accuracy**: Correct pixel classifications

**Lane-Level Metrics**:
- **ğŸ“ Lane Detection Rate**: Percentage of correctly detected lanes
- **ğŸ“ Position Accuracy**: Distance error in lane center estimation
- **ğŸš— False Positive Rate**: Incorrectly detected lane segments

### Optimization Strategies

**Model Improvements**:
1. **ğŸ”§ Architecture**: Add more encoder/decoder levels
2. **ğŸ“Š Data Augmentation**: Rotation, brightness, contrast variations
3. **âš–ï¸ Loss Functions**: Focal loss for class imbalance
4. **ğŸ¯ Post-processing**: Morphological operations for cleaner masks

**Performance Tuning**:
1. **âš¡ Mixed Precision**: Use float16 for faster training
2. **ğŸ“± Model Pruning**: Remove redundant parameters
3. **ğŸ”„ Knowledge Distillation**: Transfer from larger models
4. **ğŸ“Š Quantization**: Int8 inference for deployment

## ğŸ› ï¸ Customization

### Adapting for Different Datasets

**Dataset Integration Steps**:
1. **ğŸ“Š Format Analysis**: Understand annotation structure
2. **ğŸ”„ Loader Modification**: Adapt `__getitem__` method
3. **ğŸ“ Resolution Adjustment**: Update image dimensions
4. **ğŸ¨ Visualization**: Modify display functions

**Example: CULANE Dataset**:
```python
class CULaneDataset(Dataset):
    def __getitem__(self, idx):
        # Load image
        img = cv2.imread(self.image_paths[idx])
        
        # Load segmentation mask (different format)
        mask = cv2.imread(self.mask_paths[idx], 0)  # Grayscale
        
        # Apply same transforms
        if self.transform:
            img = self.transform(img)
            mask = self.transform(mask)
            
        return img, mask
```

### Model Architecture Variations

**Enhanced U-Net Options**:
1. **ğŸ—ï¸ Deeper Networks**: More encoder/decoder levels
2. **ğŸ”€ Attention Mechanisms**: Focus on relevant features
3. **ğŸ“Š Batch Normalization**: Improve training stability
4. **ğŸ¯ Dense Connections**: DenseNet-style feature reuse

**Alternative Architectures**:
- **ğŸš€ DeepLab**: Atrous convolutions for multi-scale features
- **âš¡ ENet**: Lightweight architecture for real-time inference
- **ğŸ¯ SCNN**: Spatial CNN for lane-specific features

### Deployment Considerations

**Production Optimization**:
1. **ğŸ“± Model Conversion**: ONNX, TensorRT, Core ML
2. **âš¡ Inference Speed**: Batch processing, async execution
3. **ğŸ’¾ Memory Management**: Efficient tensor operations
4. **ğŸ”§ Error Handling**: Robust input validation

**Integration Example**:
```python
class LaneDetector:
    def __init__(self, model_path):
        self.model = SimpleUNet()
        self.model.load_state_dict(torch.load(model_path))
        self.model.eval()
        
    def detect_lanes(self, image):
        # Preprocess input
        processed = self.preprocess(image)
        
        # Run inference
        with torch.no_grad():
            prediction = self.model(processed)
            
        # Post-process output
        lanes = self.postprocess(prediction)
        return lanes
```

## ğŸ“– References

### Academic Papers
- **U-Net**: Ronneberger, O., et al. "U-Net: Convolutional Networks for Biomedical Image Segmentation." MICCAI 2015.
- **TuSimple**: TuSimple. "Towards End-to-End Lane Detection: An Instance Segmentation Approach." ArXiv 2018.

### Technical Resources
- **PyTorch Documentation**: [pytorch.org](https://pytorch.org/docs/)
- **Computer Vision**: [OpenCV Tutorials](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html)
- **Deep Learning**: [Deep Learning Book](https://www.deeplearningbook.org/)

### Dataset Resources
- **TuSimple Benchmark**: [GitHub Repository](https://github.com/TuSimple/tusimple-benchmark)
- **CULANE Dataset**: [CULANE](https://xingangpan.github.io/projects/CULane.html)
- **BDD100K**: [Berkeley DeepDrive](https://bdd-data.berkeley.edu/)

---

## ğŸ¤ Contributing

We welcome contributions! Areas for improvement:

- **ğŸ”§ Model architectures**: Implement attention mechanisms
- **ğŸ“Š Evaluation metrics**: Add comprehensive benchmarking
- **ğŸ¨ Visualization**: Enhanced result display
- **ğŸ“š Documentation**: Tutorial notebooks and examples

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **TuSimple** for providing the benchmark dataset
- **U-Net authors** for the foundational architecture
- **PyTorch community** for excellent deep learning framework
- **OpenCV** for computer vision utilities

---

<div align="center">

**Built with â¤ï¸ for autonomous vehicle research**

[â­ Star this repo](https://github.com/yourusername/lane-detection) â€¢ [ğŸ› Report issues](https://github.com/yourusername/lane-detection/issues) â€¢ [ğŸ’¡ Request features](https://github.com/yourusername/lane-detection/pulls)

</div>
